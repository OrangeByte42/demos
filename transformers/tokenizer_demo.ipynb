{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c0a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ALL_PROXY=http://127.0.0.1:7890\n",
      "env: HTTP_PROXY=http://127.0.0.1:7890\n",
      "env: HTTPS_PROXY=http://127.0.0.1:7890\n"
     ]
    }
   ],
   "source": [
    "%env ALL_PROXY=http://127.0.0.1:7890\n",
    "%env HTTP_PROXY=http://127.0.0.1:7890\n",
    "%env HTTPS_PROXY=http://127.0.0.1:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d374b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HUB_CACHE=./data/hf_cache\n"
     ]
    }
   ],
   "source": [
    "%env HF_HUB_CACHE=./data/hf_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0ba94",
   "metadata": {},
   "source": [
    "# Tokenizer's basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ce39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aec993f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"To be or not to be, this is a question.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa656872",
   "metadata": {},
   "source": [
    "## Step 01 : Loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267b105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading from huggingface, input the model's name, then can load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a189eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./outs/temp_tokenizer/tokenizer_config.json',\n",
       " './outs/temp_tokenizer/special_tokens_map.json',\n",
       " './outs/temp_tokenizer/vocab.txt',\n",
       " './outs/temp_tokenizer/added_tokens.json',\n",
       " './outs/temp_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the tokenizer to the local disk\n",
    "tokenizer.save_pretrained(\"./outs/temp_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10205a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='./outs/temp_tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading from the local disk\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./outs/temp_tokenizer\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dd714",
   "metadata": {},
   "source": [
    "## Step 02 : Tokenizing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19983a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'be', 'or', 'not', 'to', 'be', ',', 'this', 'is', 'a', 'question', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3165d0",
   "metadata": {},
   "source": [
    "## Step 03 : Show the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33f8907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inaugural': 7725,\n",
       " '##ener': 24454,\n",
       " 'bells': 10118,\n",
       " '[unused939]': 944,\n",
       " 'orderly': 23589,\n",
       " '[unused33]': 34,\n",
       " 'competed': 3879,\n",
       " 'dil': 29454,\n",
       " 'ale': 15669,\n",
       " 'tai': 13843,\n",
       " 'deprived': 17676,\n",
       " 'skinned': 19937,\n",
       " 'sidewalk': 11996,\n",
       " 'elsie': 24603,\n",
       " 'ァ': 1692,\n",
       " '##dm': 22117,\n",
       " '[unused598]': 603,\n",
       " '##imov': 25299,\n",
       " 'realization': 12393,\n",
       " '##sell': 23836,\n",
       " 'sparkled': 28092,\n",
       " 'zu': 16950,\n",
       " '和': 1796,\n",
       " '疒': 1913,\n",
       " 'deficit': 15074,\n",
       " 'lyric': 13677,\n",
       " 'nursery': 13640,\n",
       " 'span': 8487,\n",
       " 'citizen': 6926,\n",
       " 'ং': 1346,\n",
       " 'contributors': 16884,\n",
       " 'themselves': 3209,\n",
       " 'residue': 21755,\n",
       " '##int': 18447,\n",
       " '##tung': 21847,\n",
       " '157': 17403,\n",
       " 'chords': 18495,\n",
       " 'exploit': 18077,\n",
       " 'auditioned': 23008,\n",
       " 'famously': 18172,\n",
       " 'strata': 22913,\n",
       " 'muscular': 13472,\n",
       " 'reese': 15883,\n",
       " 'mph': 5601,\n",
       " 'rectory': 24606,\n",
       " 'adjunct': 20621,\n",
       " 'serpent': 16517,\n",
       " 'diego': 5277,\n",
       " '##ggs': 21314,\n",
       " 'polished': 12853,\n",
       " 'bidding': 17534,\n",
       " 'limp': 14401,\n",
       " 'refurbishment': 24478,\n",
       " 'privacy': 9394,\n",
       " '##yle': 12844,\n",
       " 'boundaries': 7372,\n",
       " 'auf': 21200,\n",
       " 'sued': 12923,\n",
       " '##x': 2595,\n",
       " 'defined': 4225,\n",
       " 'blouse': 18149,\n",
       " 'forrest': 16319,\n",
       " 'tobago': 17247,\n",
       " '##alle': 24164,\n",
       " 'rushed': 6760,\n",
       " '##囗': 30323,\n",
       " 'nestled': 22704,\n",
       " 'borders': 6645,\n",
       " 'outlaw': 19104,\n",
       " 'collaborating': 20295,\n",
       " '##rrie': 22155,\n",
       " 'antilles': 27695,\n",
       " 'skeptical': 18386,\n",
       " 'sebastian': 6417,\n",
       " 'cattle': 7125,\n",
       " 'evan': 9340,\n",
       " '##宇': 30345,\n",
       " 'lynx': 22636,\n",
       " '##urt': 19585,\n",
       " 'carved': 7844,\n",
       " '##ᄊ': 29998,\n",
       " 'cairns': 21731,\n",
       " 'surreal': 16524,\n",
       " 'pl': 20228,\n",
       " 'martinez': 10337,\n",
       " 'mission': 3260,\n",
       " '##fera': 27709,\n",
       " 'consciousness': 8298,\n",
       " 'foreman': 18031,\n",
       " '##chase': 26300,\n",
       " 'initiate': 17820,\n",
       " 'confines': 25722,\n",
       " 'decorations': 14529,\n",
       " 'shortage': 15843,\n",
       " '##谷': 30477,\n",
       " '[unused140]': 145,\n",
       " '##phones': 19093,\n",
       " '##42': 20958,\n",
       " 'accounting': 9529,\n",
       " 'permitting': 24523,\n",
       " 'intricate': 17796,\n",
       " '##nist': 26942,\n",
       " '##haw': 14238,\n",
       " 'speaker': 5882,\n",
       " 'josephine': 16117,\n",
       " 'ensued': 18942,\n",
       " 'minorities': 14302,\n",
       " '[unused814]': 819,\n",
       " '##cent': 13013,\n",
       " '##rb': 15185,\n",
       " 'eyelashes': 25150,\n",
       " '##wheel': 22920,\n",
       " 'τ': 1174,\n",
       " 'lateral': 11457,\n",
       " 'crossings': 20975,\n",
       " 'false': 6270,\n",
       " 'astor': 25159,\n",
       " 'starboard': 25211,\n",
       " 'soared': 29127,\n",
       " '##ivision': 24607,\n",
       " 'soho': 23771,\n",
       " 'theatres': 13166,\n",
       " 'integrate': 17409,\n",
       " 'blake': 6511,\n",
       " 'astrid': 27376,\n",
       " 'enduring': 16762,\n",
       " 'sipped': 17735,\n",
       " 'baja': 19497,\n",
       " 'c2': 29248,\n",
       " 'glorious': 14013,\n",
       " 'conclusion': 7091,\n",
       " '##comb': 18274,\n",
       " 'minnesota': 5135,\n",
       " 'enables': 12939,\n",
       " 'passionate': 13459,\n",
       " '##minating': 27932,\n",
       " '##emi': 23238,\n",
       " '[unused587]': 592,\n",
       " 'william': 2520,\n",
       " 'compatible': 11892,\n",
       " '##86': 20842,\n",
       " '##par': 19362,\n",
       " 'guantanamo': 23094,\n",
       " 'tempest': 22553,\n",
       " 'julie': 7628,\n",
       " '1954': 4051,\n",
       " '##mu': 12274,\n",
       " '##burgh': 15496,\n",
       " 'engagement': 8147,\n",
       " 'romantic': 6298,\n",
       " '[unused580]': 585,\n",
       " 'stimulation': 20858,\n",
       " 'disappointed': 9364,\n",
       " 'precisely': 10785,\n",
       " '2018': 2760,\n",
       " 'thrill': 16959,\n",
       " 'arising': 17707,\n",
       " 'modulation': 25502,\n",
       " '[unused38]': 39,\n",
       " 'twitter': 10474,\n",
       " '##no': 3630,\n",
       " 'musicians': 5389,\n",
       " 'routes': 5847,\n",
       " 'gilbert': 7664,\n",
       " 'atkinson': 18646,\n",
       " '##lford': 23211,\n",
       " 'eyebrows': 8407,\n",
       " 'promotion': 4712,\n",
       " 'ti': 14841,\n",
       " 'atlantic': 4448,\n",
       " 'revised': 8001,\n",
       " 'miracles': 17861,\n",
       " 'adolescent': 20274,\n",
       " 'leopold': 12752,\n",
       " 'fe': 10768,\n",
       " 'colossal': 29523,\n",
       " 'urbana': 27929,\n",
       " 'afar': 28978,\n",
       " '##ize': 4697,\n",
       " 'beat': 3786,\n",
       " 'cries': 12842,\n",
       " '##ido': 13820,\n",
       " 'southwest': 4943,\n",
       " 'practitioners': 14617,\n",
       " 'perception': 10617,\n",
       " 'groceries': 26298,\n",
       " '##opped': 27288,\n",
       " 'telecast': 28803,\n",
       " 'playground': 14705,\n",
       " '##acle': 18630,\n",
       " 'angelo': 12262,\n",
       " 'chuckle': 15375,\n",
       " 'position': 2597,\n",
       " 'aidan': 12643,\n",
       " '##lassified': 24938,\n",
       " '##mber': 21784,\n",
       " 'suicide': 5920,\n",
       " 'sincerely': 25664,\n",
       " 'complement': 13711,\n",
       " '##itis': 13706,\n",
       " '##der': 4063,\n",
       " 'antigen': 28873,\n",
       " '##jad': 27875,\n",
       " '##eta': 12928,\n",
       " 'leases': 29597,\n",
       " 'baptised': 28798,\n",
       " 'mellon': 22181,\n",
       " '[unused759]': 764,\n",
       " 'kings': 5465,\n",
       " 'nielsen': 13188,\n",
       " '##keepers': 24764,\n",
       " 'strains': 18859,\n",
       " 'cliff': 7656,\n",
       " 'middletown': 28747,\n",
       " 'property': 3200,\n",
       " 'demise': 13614,\n",
       " 'aquitaine': 24973,\n",
       " 'kata': 29354,\n",
       " 'atlas': 11568,\n",
       " 'seasonal': 12348,\n",
       " 'voters': 7206,\n",
       " 'seeks': 11014,\n",
       " 'bethel': 19375,\n",
       " 'estimates': 10035,\n",
       " 'partridge': 26079,\n",
       " 'asshole': 22052,\n",
       " 'ballad': 11571,\n",
       " '55th': 29075,\n",
       " 'ignition': 17446,\n",
       " 'archangel': 28185,\n",
       " '##mler': 18602,\n",
       " '⁻': 1545,\n",
       " 'advised': 9449,\n",
       " 'umm': 26114,\n",
       " 'callahan': 25668,\n",
       " 'achilles': 23167,\n",
       " '##alla': 25425,\n",
       " 'servant': 7947,\n",
       " 'dictionary': 9206,\n",
       " '##ima': 9581,\n",
       " 'females': 3801,\n",
       " 'lighting': 7497,\n",
       " 'masterpiece': 17743,\n",
       " '##ods': 20620,\n",
       " '##hor': 16368,\n",
       " 'm1': 23290,\n",
       " '[unused257]': 262,\n",
       " 'ass': 4632,\n",
       " '##tori': 29469,\n",
       " 'nomination': 6488,\n",
       " 'cup': 2452,\n",
       " 'originals': 23728,\n",
       " '##wife': 19993,\n",
       " '##aged': 18655,\n",
       " 'sense': 3168,\n",
       " 'charlemagne': 27257,\n",
       " 'gm': 13938,\n",
       " 'snoop': 29044,\n",
       " 'designations': 26672,\n",
       " '##ically': 15004,\n",
       " 'socrates': 26772,\n",
       " 'abc': 5925,\n",
       " 'biennial': 20313,\n",
       " 'sensations': 21378,\n",
       " '##life': 15509,\n",
       " 'jaws': 16113,\n",
       " 'foreign': 3097,\n",
       " 'ia': 24264,\n",
       " '##ratic': 23671,\n",
       " '1758': 16832,\n",
       " 'walk': 3328,\n",
       " 'connected': 4198,\n",
       " 'instrumental': 6150,\n",
       " '##cial': 13247,\n",
       " 'ろ': 1689,\n",
       " '##riam': 25557,\n",
       " '[unused384]': 389,\n",
       " 'establishing': 7411,\n",
       " 'conflicts': 9755,\n",
       " '##kura': 28260,\n",
       " 'potassium': 18044,\n",
       " 'mortals': 23844,\n",
       " 'revealing': 8669,\n",
       " 'explicitly': 12045,\n",
       " '##phine': 20738,\n",
       " '##tead': 14565,\n",
       " 'gunner': 14850,\n",
       " 'announcements': 25674,\n",
       " '70': 3963,\n",
       " 'comrades': 19033,\n",
       " 'collapse': 7859,\n",
       " '[unused360]': 365,\n",
       " '##ographic': 13705,\n",
       " 'manual': 6410,\n",
       " '[unused923]': 928,\n",
       " 'encompassing': 19129,\n",
       " 'peptide': 25117,\n",
       " '男': 1912,\n",
       " 'crypt': 19888,\n",
       " 'mirror': 5259,\n",
       " '[unused136]': 141,\n",
       " 'songs': 2774,\n",
       " 'nominee': 9773,\n",
       " '##ti': 3775,\n",
       " '##も': 30207,\n",
       " 'maintains': 9319,\n",
       " 'villa': 6992,\n",
       " 'caleb': 10185,\n",
       " 'declaration': 8170,\n",
       " 'fears': 10069,\n",
       " '##adia': 25205,\n",
       " 'regis': 20588,\n",
       " '##erin': 23282,\n",
       " 'bike': 7997,\n",
       " '##zak': 20685,\n",
       " 'whereabouts': 18913,\n",
       " 'chan': 9212,\n",
       " 'shirley': 11280,\n",
       " 'occasions': 6642,\n",
       " '1963': 3699,\n",
       " 'viewed': 7021,\n",
       " 'heidi': 21372,\n",
       " '[unused548]': 553,\n",
       " 'lash': 25210,\n",
       " 'ш': 1203,\n",
       " '##ont': 12162,\n",
       " 'taiwanese': 16539,\n",
       " '##rner': 18703,\n",
       " '##ab': 7875,\n",
       " 'sofia': 8755,\n",
       " '[unused137]': 142,\n",
       " 'addict': 26855,\n",
       " '1924': 4814,\n",
       " '##rbin': 27366,\n",
       " '##ller': 10820,\n",
       " 'fugitive': 21329,\n",
       " '↦': 1588,\n",
       " 'harrisburg': 24569,\n",
       " 'ェ': 1697,\n",
       " 'strive': 29453,\n",
       " 'reinforcement': 23895,\n",
       " '##oning': 13369,\n",
       " 'crown': 4410,\n",
       " 'pri': 26927,\n",
       " 'confidence': 7023,\n",
       " '##lea': 19738,\n",
       " '##piece': 11198,\n",
       " 'saigon': 24001,\n",
       " 'hugely': 27564,\n",
       " 'lucrative': 21115,\n",
       " '150': 5018,\n",
       " '##odes': 19847,\n",
       " 'depot': 8470,\n",
       " '##tz': 5753,\n",
       " '##vat': 22879,\n",
       " '##！': 30512,\n",
       " 'revisited': 24354,\n",
       " '##kei': 29501,\n",
       " 'খ': 1354,\n",
       " 'growls': 27825,\n",
       " 'behaviour': 9164,\n",
       " 'b': 1038,\n",
       " '##mpton': 26793,\n",
       " '1724': 26423,\n",
       " 'devi': 14386,\n",
       " 'roxy': 23682,\n",
       " 'circulating': 22458,\n",
       " 'newfound': 27608,\n",
       " 'guiana': 23568,\n",
       " 'actions': 4506,\n",
       " '##語': 30476,\n",
       " 'mainland': 8240,\n",
       " 'greedy': 20505,\n",
       " 'dartmouth': 16960,\n",
       " '##ი': 29981,\n",
       " '[unused739]': 744,\n",
       " 'inhaled': 15938,\n",
       " 'landing': 4899,\n",
       " 'medallion': 22541,\n",
       " 'economics': 5543,\n",
       " '##rum': 6824,\n",
       " '[unused945]': 950,\n",
       " 'gesture': 9218,\n",
       " 'elderly': 9750,\n",
       " '##tus': 5809,\n",
       " '##tou': 24826,\n",
       " 'lane': 4644,\n",
       " '″': 1532,\n",
       " 'pete': 6969,\n",
       " '##por': 17822,\n",
       " 'romani': 23982,\n",
       " 'battleships': 21327,\n",
       " '##teacher': 24741,\n",
       " 'strain': 10178,\n",
       " 'iss': 26354,\n",
       " 'romance': 7472,\n",
       " 'lung': 11192,\n",
       " 'drug': 4319,\n",
       " 'elemental': 19529,\n",
       " 'benign': 28378,\n",
       " '[unused425]': 430,\n",
       " 'lantern': 12856,\n",
       " '[unused188]': 193,\n",
       " 'medina': 15761,\n",
       " 'mary': 2984,\n",
       " 'ties': 7208,\n",
       " 'techniques': 5461,\n",
       " 'interface': 8278,\n",
       " 'elgin': 23792,\n",
       " 'emphasizing': 22671,\n",
       " '1805': 13126,\n",
       " 'treated': 5845,\n",
       " 'algorithm': 9896,\n",
       " 'wat': 28194,\n",
       " '##erty': 15010,\n",
       " 'brother': 2567,\n",
       " '##hae': 25293,\n",
       " '##rked': 19849,\n",
       " 'resignation': 8172,\n",
       " 'danielle': 18490,\n",
       " 'clarke': 8359,\n",
       " 'brunei': 18692,\n",
       " 'applicable': 12711,\n",
       " 'burnett': 13829,\n",
       " 'curate': 27530,\n",
       " 'raced': 8255,\n",
       " '[unused983]': 988,\n",
       " 'chad': 9796,\n",
       " '1634': 28502,\n",
       " '##right': 15950,\n",
       " 'niagara': 15473,\n",
       " 'story': 2466,\n",
       " 'caesar': 11604,\n",
       " 'accelerate': 23306,\n",
       " '1730': 23272,\n",
       " '##ple': 10814,\n",
       " 'sweater': 14329,\n",
       " 'groundbreaking': 23222,\n",
       " 'alfredo': 19423,\n",
       " '##瀬': 30431,\n",
       " 'nsa': 23971,\n",
       " 'priesthood': 17911,\n",
       " '##hat': 12707,\n",
       " 'crumpled': 19814,\n",
       " 'sporadic': 24590,\n",
       " '[unused220]': 225,\n",
       " 'thierry': 26413,\n",
       " 'exhibits': 10637,\n",
       " 'imprint': 15738,\n",
       " 'strategic': 6143,\n",
       " 'lindsey': 17518,\n",
       " 'elves': 16980,\n",
       " '##ifies': 14144,\n",
       " '##rim': 20026,\n",
       " 'ᅦ': 1473,\n",
       " 'alice': 5650,\n",
       " 'collaborations': 17437,\n",
       " '##hrer': 17875,\n",
       " '670': 25535,\n",
       " 'mesa': 15797,\n",
       " 'accelerating': 29494,\n",
       " 'capitals': 15433,\n",
       " '版': 1907,\n",
       " 'fluttered': 18360,\n",
       " '##文': 30387,\n",
       " 'silvia': 27827,\n",
       " 'haifa': 21303,\n",
       " 'oskar': 28626,\n",
       " 'beds': 9705,\n",
       " 'ucla': 12389,\n",
       " 'い': 1647,\n",
       " 'provisional': 10864,\n",
       " '##sol': 19454,\n",
       " 'reformation': 13708,\n",
       " 'mohawk': 22338,\n",
       " 'emilia': 20417,\n",
       " 'sprayed': 25401,\n",
       " '1987': 3055,\n",
       " 'conner': 17639,\n",
       " 'emmanuel': 14459,\n",
       " '##rdial': 25070,\n",
       " 'unanimously': 15645,\n",
       " 'pascal': 17878,\n",
       " 'composed': 3605,\n",
       " 'buy': 4965,\n",
       " 'singled': 25369,\n",
       " 'transports': 19003,\n",
       " 'selling': 4855,\n",
       " 'ritchie': 20404,\n",
       " '114': 12457,\n",
       " '##smo': 25855,\n",
       " 'elm': 17709,\n",
       " '[unused313]': 318,\n",
       " 'impoverished': 25488,\n",
       " 'delightful': 26380,\n",
       " 'harriet': 14207,\n",
       " 'amara': 28599,\n",
       " 'gene': 4962,\n",
       " 'superseded': 19886,\n",
       " 'hotspur': 25985,\n",
       " 'sizable': 25908,\n",
       " '##bai': 26068,\n",
       " 'duchy': 11068,\n",
       " '##ched': 7690,\n",
       " 'homogeneous': 24854,\n",
       " 'cpu': 17368,\n",
       " 'ark': 15745,\n",
       " 'stephenson': 19789,\n",
       " 'umpire': 20887,\n",
       " '##ℓ': 30105,\n",
       " 'boasted': 23390,\n",
       " 'crimes': 6997,\n",
       " 'shook': 3184,\n",
       " 'vilnius': 20513,\n",
       " 'brothers': 3428,\n",
       " 'targets': 7889,\n",
       " 'rune': 23276,\n",
       " 'scholastic': 24105,\n",
       " 'pear': 28253,\n",
       " 'altitude': 7998,\n",
       " 'boxing': 8362,\n",
       " 'shattering': 21797,\n",
       " 'moonlight': 11986,\n",
       " 'liam': 8230,\n",
       " 'pickup': 15373,\n",
       " 'maker': 9338,\n",
       " 'sideways': 12579,\n",
       " 'cardiff': 10149,\n",
       " 'rhodesia': 20340,\n",
       " 'reserve': 3914,\n",
       " '##hab': 25459,\n",
       " '##vs': 15088,\n",
       " 'shouldered': 29383,\n",
       " 'polynomial': 17505,\n",
       " 'energy': 2943,\n",
       " 'bored': 11471,\n",
       " 'biting': 12344,\n",
       " 'aba': 19557,\n",
       " 'metabolic': 21453,\n",
       " 'started': 2318,\n",
       " 'ticking': 28561,\n",
       " 'reacts': 27325,\n",
       " '0': 1014,\n",
       " '1845': 9512,\n",
       " 'campus': 3721,\n",
       " 'tended': 11121,\n",
       " 'walls': 3681,\n",
       " 'stacey': 19997,\n",
       " 'emma': 5616,\n",
       " 'sandwich': 11642,\n",
       " 'glasgow': 6785,\n",
       " 'playfully': 22608,\n",
       " 'accomplishment': 24718,\n",
       " 'turkic': 22926,\n",
       " 'allie': 16944,\n",
       " 'walt': 10598,\n",
       " 'of': 1997,\n",
       " '##aur': 21159,\n",
       " 'phases': 12335,\n",
       " '雄': 1974,\n",
       " 'pulse': 8187,\n",
       " 'scenario': 11967,\n",
       " 'iowa': 5947,\n",
       " 'reagan': 11531,\n",
       " 'stereo': 12991,\n",
       " 'smallpox': 25765,\n",
       " '戦': 1856,\n",
       " 'utterly': 12580,\n",
       " 'pensacola': 26073,\n",
       " '##ds': 5104,\n",
       " '##tarian': 28897,\n",
       " 'annals': 16945,\n",
       " 'governed': 9950,\n",
       " 'entertain': 20432,\n",
       " '##cking': 23177,\n",
       " 'jumper': 21097,\n",
       " '##mc': 12458,\n",
       " '##ᄋ': 29999,\n",
       " 'germany': 2762,\n",
       " 'vacant': 10030,\n",
       " '##pate': 17585,\n",
       " 'advancement': 12607,\n",
       " '##yuki': 19663,\n",
       " 'pulled': 2766,\n",
       " 'leather': 5898,\n",
       " '##boys': 24916,\n",
       " 'ions': 15956,\n",
       " '1756': 22370,\n",
       " 'ultrasound': 27312,\n",
       " 'coat': 5435,\n",
       " 'heated': 9685,\n",
       " '[unused451]': 456,\n",
       " 'choke': 16769,\n",
       " 'op': 6728,\n",
       " 'liberia': 18039,\n",
       " 'signifies': 27353,\n",
       " 'wong': 11789,\n",
       " 'ky': 18712,\n",
       " 'banker': 13448,\n",
       " 'speak': 3713,\n",
       " '[unused349]': 354,\n",
       " 'rye': 20926,\n",
       " '##kari': 20224,\n",
       " 'stalking': 20070,\n",
       " 'wheels': 7787,\n",
       " 'commissioners': 12396,\n",
       " 'elections': 3864,\n",
       " 'immense': 14269,\n",
       " 'horses': 5194,\n",
       " 'rufus': 18316,\n",
       " 'colonies': 8355,\n",
       " 'stocked': 24802,\n",
       " 'publishing': 4640,\n",
       " 'bertrand': 20586,\n",
       " '##agger': 27609,\n",
       " 'scratches': 25980,\n",
       " 'require': 5478,\n",
       " 'robinson': 6157,\n",
       " 'implementing': 14972,\n",
       " 'somewhat': 5399,\n",
       " '##tas': 10230,\n",
       " '##cs': 6169,\n",
       " '[unused531]': 536,\n",
       " 'peel': 14113,\n",
       " '##bana': 19445,\n",
       " 'intervening': 26623,\n",
       " 'weekdays': 19759,\n",
       " 'thicker': 19638,\n",
       " '##宣': 30349,\n",
       " 'sant': 15548,\n",
       " '##iy': 28008,\n",
       " 'bee': 10506,\n",
       " 'newmarket': 22489,\n",
       " 'hands': 2398,\n",
       " '##cius': 25393,\n",
       " 'become': 2468,\n",
       " 'olympic': 4386,\n",
       " '\\\\': 1032,\n",
       " 'ada': 15262,\n",
       " '##ctric': 22601,\n",
       " 'rejoined': 14311,\n",
       " '##zone': 15975,\n",
       " '##cula': 19879,\n",
       " '##sen': 5054,\n",
       " 'poorly': 9996,\n",
       " 'gulped': 25411,\n",
       " 'himalayas': 26779,\n",
       " 'leiden': 20329,\n",
       " 'jury': 6467,\n",
       " 'strung': 23509,\n",
       " 'sees': 5927,\n",
       " 'santana': 21158,\n",
       " 'duck': 9457,\n",
       " 'mathew': 25436,\n",
       " 'ceasefire': 26277,\n",
       " 'cola': 15270,\n",
       " 'pierce': 9267,\n",
       " 'sentiment': 15792,\n",
       " 'recital': 25521,\n",
       " 'minors': 18464,\n",
       " 'shutter': 28180,\n",
       " 'rue': 13413,\n",
       " 'frontal': 19124,\n",
       " '##eson': 21421,\n",
       " '##ბ': 29975,\n",
       " 'რ': 1451,\n",
       " '##52': 25746,\n",
       " 'pasture': 20787,\n",
       " 'ا': 1270,\n",
       " 'kneeling': 16916,\n",
       " '[unused764]': 769,\n",
       " '##jou': 23099,\n",
       " 'revolutionary': 6208,\n",
       " 'microwave': 18302,\n",
       " 'empress': 10248,\n",
       " 'assign': 23911,\n",
       " 'sentai': 28650,\n",
       " '##aghan': 26685,\n",
       " 'ate': 8823,\n",
       " '##dication': 25027,\n",
       " 'array': 9140,\n",
       " 'dumb': 12873,\n",
       " 'strides': 22215,\n",
       " 'gora': 26967,\n",
       " 'curtiss': 26431,\n",
       " 'interval': 13483,\n",
       " 'themes': 6991,\n",
       " '##eased': 25063,\n",
       " 'geologist': 21334,\n",
       " '##dhi': 19114,\n",
       " 'parasites': 23996,\n",
       " 'morgan': 5253,\n",
       " 'bordered': 11356,\n",
       " 'lange': 21395,\n",
       " 'striking': 8478,\n",
       " '##ᵖ': 30040,\n",
       " 'evident': 10358,\n",
       " 'rotated': 20931,\n",
       " 'xml': 20950,\n",
       " 'alf': 24493,\n",
       " 'squares': 14320,\n",
       " 'rotary': 16933,\n",
       " '1716': 28204,\n",
       " '##gill': 19791,\n",
       " 'outspoken': 22430,\n",
       " 'stance': 11032,\n",
       " 'leighton': 26873,\n",
       " 'critical': 4187,\n",
       " 'khalid': 21828,\n",
       " 'remained': 2815,\n",
       " '##nies': 15580,\n",
       " 'priced': 21125,\n",
       " 'sneaking': 20727,\n",
       " 'dona': 24260,\n",
       " 'privateer': 26790,\n",
       " 'psalm': 22728,\n",
       " 'bobbed': 29579,\n",
       " 'vivo': 24269,\n",
       " 'mongols': 22235,\n",
       " 'dina': 26146,\n",
       " 'threshold': 11207,\n",
       " '##mania': 27010,\n",
       " 'azerbaijani': 18325,\n",
       " 'distracting': 25012,\n",
       " 'repertoire': 13646,\n",
       " 'sheds': 25999,\n",
       " 'forest': 3224,\n",
       " 'developers': 9797,\n",
       " 'kan': 22827,\n",
       " 'decides': 7288,\n",
       " 'vs': 5443,\n",
       " 'scales': 9539,\n",
       " 'ensemble': 7241,\n",
       " 'jars': 25067,\n",
       " 'companions': 11946,\n",
       " '##nz': 14191,\n",
       " '1723': 26621,\n",
       " 'trips': 9109,\n",
       " 'ibm': 9980,\n",
       " 'badge': 10780,\n",
       " 'evolving': 20607,\n",
       " '[unused426]': 431,\n",
       " 'justine': 26377,\n",
       " '[unused422]': 427,\n",
       " 'tudor': 15588,\n",
       " 'tricked': 24929,\n",
       " '発': 1914,\n",
       " 'aa': 9779,\n",
       " '90s': 17233,\n",
       " '##ـ': 29832,\n",
       " 'marred': 24563,\n",
       " 'vice': 3580,\n",
       " 'bravery': 16534,\n",
       " 'royal': 2548,\n",
       " '##grate': 22780,\n",
       " 'squeak': 29552,\n",
       " '##ami': 10631,\n",
       " 'painting': 4169,\n",
       " 'dispatch': 18365,\n",
       " 'physically': 8186,\n",
       " '##iste': 27870,\n",
       " 'healing': 8907,\n",
       " 'forcing': 6932,\n",
       " 'ג': 1243,\n",
       " '##shaw': 17980,\n",
       " '1680': 24621,\n",
       " 'reduces': 13416,\n",
       " 'excellent': 6581,\n",
       " 'parent': 6687,\n",
       " 'guinness': 17323,\n",
       " 'floated': 13715,\n",
       " '128': 11899,\n",
       " 'creeping': 18266,\n",
       " 'exported': 15612,\n",
       " '[unused533]': 538,\n",
       " 'gives': 3957,\n",
       " 'sham': 25850,\n",
       " 'majority': 3484,\n",
       " 'ecology': 13517,\n",
       " '##ilis': 24411,\n",
       " 'lest': 26693,\n",
       " '##achal': 24409,\n",
       " 'ィ': 1694,\n",
       " 'ball': 3608,\n",
       " 'readily': 12192,\n",
       " 'distinctive': 8200,\n",
       " 'core': 4563,\n",
       " 'lana': 16554,\n",
       " 'knight': 5000,\n",
       " 'three': 2093,\n",
       " 'concepcion': 27331,\n",
       " 'privileges': 14310,\n",
       " 'farther': 8736,\n",
       " 'radha': 26498,\n",
       " 'ratified': 17673,\n",
       " 'stared': 3592,\n",
       " 'dominion': 13738,\n",
       " '285': 21777,\n",
       " 'þ': 1101,\n",
       " 'rows': 10281,\n",
       " 'later': 2101,\n",
       " 'spill': 14437,\n",
       " 'thirds': 12263,\n",
       " '##apa': 22068,\n",
       " '38': 4229,\n",
       " 'controversial': 6801,\n",
       " 'goats': 17977,\n",
       " 'hauled': 13161,\n",
       " '##ented': 14088,\n",
       " 'asteroid': 12175,\n",
       " '##ald': 19058,\n",
       " 'greeted': 11188,\n",
       " '##erik': 27350,\n",
       " 'demand': 5157,\n",
       " 'cheerful': 18350,\n",
       " 'sprawled': 21212,\n",
       " '##hl': 7317,\n",
       " 'okinawa': 15052,\n",
       " 'pokemon': 20421,\n",
       " 'bolivar': 22118,\n",
       " 'nova': 6846,\n",
       " '##ನ': 29936,\n",
       " 'townland': 23635,\n",
       " 'breasts': 12682,\n",
       " '##anum': 27975,\n",
       " 'wendell': 24526,\n",
       " '##yde': 18124,\n",
       " '##don': 5280,\n",
       " 'vladimir': 8748,\n",
       " 'pursuit': 8463,\n",
       " '229': 22777,\n",
       " 'colombia': 7379,\n",
       " 'nope': 16780,\n",
       " 'clan': 6338,\n",
       " '##orted': 15613,\n",
       " 'molten': 23548,\n",
       " 'littered': 24777,\n",
       " 'magistrates': 23007,\n",
       " 'duke': 3804,\n",
       " 'nixon': 11296,\n",
       " 'employing': 15440,\n",
       " 'plotted': 27347,\n",
       " 'bolivian': 26075,\n",
       " 'tanks': 7286,\n",
       " 'recommendation': 12832,\n",
       " 'modifying': 29226,\n",
       " 'shelter': 7713,\n",
       " 'abbas': 17532,\n",
       " 'healer': 19783,\n",
       " '##ogist': 22522,\n",
       " '##port': 6442,\n",
       " 'undefeated': 15188,\n",
       " '##ز': 29823,\n",
       " 'dramas': 16547,\n",
       " '##drick': 24092,\n",
       " 'restless': 15035,\n",
       " 'manufacturer': 7751,\n",
       " 'epic': 8680,\n",
       " 'muhammad': 7187,\n",
       " 'bp': 17531,\n",
       " 'bracket': 21605,\n",
       " 'arrange': 13621,\n",
       " '##uses': 25581,\n",
       " '##tness': 27401,\n",
       " '##laise': 25122,\n",
       " 'elton': 19127,\n",
       " 'cleansing': 26799,\n",
       " 'wards': 11682,\n",
       " 'held': 2218,\n",
       " 'robert': 2728,\n",
       " '[unused594]': 599,\n",
       " 'streak': 9039,\n",
       " 'runes': 29161,\n",
       " 'scored': 3195,\n",
       " 'pencil': 14745,\n",
       " 'gonzaga': 26840,\n",
       " 'alexandria': 10297,\n",
       " '##rnet': 26573,\n",
       " 'werewolf': 12797,\n",
       " 'penetrated': 21653,\n",
       " 'volcanic': 10942,\n",
       " 'neutrality': 21083,\n",
       " '34': 4090,\n",
       " '##郡': 30485,\n",
       " '##nem': 25832,\n",
       " 'м': 1191,\n",
       " 'laced': 17958,\n",
       " 'developing': 4975,\n",
       " '##oa': 10441,\n",
       " 'treason': 14712,\n",
       " 'shame': 9467,\n",
       " 'verified': 20119,\n",
       " 'circled': 14867,\n",
       " 'closet': 9346,\n",
       " 'howell': 18473,\n",
       " '335': 24426,\n",
       " 'indications': 24936,\n",
       " 'madam': 21658,\n",
       " 'fletcher': 10589,\n",
       " '##erman': 18689,\n",
       " '[unused249]': 254,\n",
       " 'characterized': 7356,\n",
       " '##rre': 14343,\n",
       " 'outskirts': 12730,\n",
       " 'hanging': 5689,\n",
       " 'erin': 11781,\n",
       " 'saxons': 28267,\n",
       " 'marked': 4417,\n",
       " '[unused617]': 622,\n",
       " 'terminology': 18444,\n",
       " '##fahan': 28975,\n",
       " 'massif': 24875,\n",
       " 'honour': 6225,\n",
       " '##vas': 12044,\n",
       " 'nfc': 22309,\n",
       " '[unused642]': 647,\n",
       " 'sewing': 22746,\n",
       " 'consider': 5136,\n",
       " 'tough': 7823,\n",
       " '##hta': 22893,\n",
       " '##ographer': 26145,\n",
       " '##ad': 4215,\n",
       " 'altogether': 10462,\n",
       " 'hearted': 18627,\n",
       " 'realising': 27504,\n",
       " 'necks': 26082,\n",
       " '[unused102]': 107,\n",
       " '[unused483]': 488,\n",
       " 'townsville': 27492,\n",
       " 'oct': 13323,\n",
       " '##leg': 23115,\n",
       " 'concerto': 10405,\n",
       " 'crumbling': 24827,\n",
       " 'bellamy': 25544,\n",
       " 'ion': 10163,\n",
       " '##א': 29788,\n",
       " 'elementary': 4732,\n",
       " 'benton': 18685,\n",
       " 'tally': 19552,\n",
       " 'cohen': 9946,\n",
       " 'abdominal': 21419,\n",
       " 'genealogy': 26684,\n",
       " 'natural': 3019,\n",
       " 'intelligent': 9414,\n",
       " 'poised': 22303,\n",
       " 'ems': 29031,\n",
       " 'urgently': 25478,\n",
       " '##chus': 16194,\n",
       " '##chemist': 24229,\n",
       " '8th': 5893,\n",
       " 'buenos': 9204,\n",
       " '[unused100]': 105,\n",
       " 'michaels': 17784,\n",
       " 'п': 1194,\n",
       " 'periods': 6993,\n",
       " 'thug': 26599,\n",
       " 'disciplines': 12736,\n",
       " 'dining': 7759,\n",
       " 'hybrids': 23376,\n",
       " '[unused716]': 721,\n",
       " '##tl': 19646,\n",
       " 'extend': 7949,\n",
       " 'eun': 26070,\n",
       " 'yahoo': 20643,\n",
       " '##অ': 29883,\n",
       " 'inscribed': 14551,\n",
       " 'sport': 4368,\n",
       " 'insult': 15301,\n",
       " 'gottingen': 23607,\n",
       " 'roberts': 7031,\n",
       " 'andhra': 14065,\n",
       " 'provided': 3024,\n",
       " 'daphne': 16847,\n",
       " '！': 1986,\n",
       " 'necessitated': 29611,\n",
       " '##dled': 20043,\n",
       " 'academics': 15032,\n",
       " 'mart': 20481,\n",
       " 'fu': 11865,\n",
       " 'hyper': 23760,\n",
       " '##led': 3709,\n",
       " '##mah': 25687,\n",
       " 'waves': 5975,\n",
       " 'similar': 2714,\n",
       " '##utz': 20267,\n",
       " '##kling': 20260,\n",
       " 'rosie': 15820,\n",
       " '1802': 13515,\n",
       " 'glad': 5580,\n",
       " '##sul': 23722,\n",
       " '##hora': 16977,\n",
       " 'endowment': 15108,\n",
       " 'assemble': 21365,\n",
       " 'bravo': 17562,\n",
       " 'grandpa': 15310,\n",
       " 'exactly': 3599,\n",
       " 'hull': 6738,\n",
       " 'floating': 8274,\n",
       " 'watford': 21740,\n",
       " 'hicks': 17221,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c3026d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9632aaa",
   "metadata": {},
   "source": [
    "## Step 04 : Mutual converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bdfe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2000, 2022, 2030, 2025, 2000, 2022, 1010, 2023, 2003, 1037, 3160, 1012]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the token to token id\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd52cd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'be', 'or', 'not', 'to', 'be', ',', 'this', 'is', 'a', 'question', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the token id to token\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccf6cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to be or not to be, this is a question.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the token to string\n",
    "str_sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6570659",
   "metadata": {},
   "source": [
    "Better ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d40fcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2000,\n",
       " 2022,\n",
       " 2030,\n",
       " 2025,\n",
       " 2000,\n",
       " 2022,\n",
       " 1010,\n",
       " 2023,\n",
       " 2003,\n",
       " 1037,\n",
       " 3160,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the string to token ids\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e50533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] to be or not to be, this is a question. [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the token ids to string\n",
    "str_sentence = tokenizer.decode(token_ids)\n",
    "str_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40baeab1",
   "metadata": {},
   "source": [
    "## Step 05 : Padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "068cf06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2000,\n",
       " 2022,\n",
       " 2030,\n",
       " 2025,\n",
       " 2000,\n",
       " 2022,\n",
       " 1010,\n",
       " 2023,\n",
       " 2003,\n",
       " 1037,\n",
       " 3160,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "token_ids = tokenizer.encode(sentence, padding='max_length', max_length=24)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "550da65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2000, 2022, 2030, 2025, 2000, 2022, 102]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncation\n",
    "token_ids = tokenizer.encode(sentence, truncation=True, max_length=8)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffd4e1",
   "metadata": {},
   "source": [
    "## Step 06 : Other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32603d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2000, 2022, 2030, 2025, 2000, 2022, 102],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in token_ids]\n",
    "token_type_ids = [0] * len(token_ids)\n",
    "token_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592d460",
   "metadata": {},
   "source": [
    "## Step 07 : Fast calling way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "328f49b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2000, 2022, 2030, 2025, 2000, 2022, 1010, 2023, 2003, 1037, 3160, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sentence, padding='max_length', max_length=24)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91169218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2000, 2022, 2030, 2025, 2000, 2022, 1010, 2023, 2003, 1037, 3160, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sentence, padding='max_length', max_length=24)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffed21",
   "metadata": {},
   "source": [
    "## Step 08 : Deal with batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "797a5b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2000, 2022, 2030, 2025, 2000, 2022, 1010, 2023, 2003, 1037, 3160, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2023, 2030, 2023, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2040, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'To be or not to be, this is a question.',\n",
    "    'This or this',\n",
    "    'Who are you ?',\n",
    "]\n",
    "\n",
    "res = tokenizer(sentences, padding='max_length', max_length=24)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb128dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92 ms, sys: 0 ns, total: 92 ms\n",
      "Wall time: 90.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(1_000):\n",
    "    tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94ec8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 11.3 ms, total: 148 ms\n",
      "Wall time: 17.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = tokenizer([sentence] * 1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc275c",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fefa2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"To be or not to be, this is a question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81f45220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", use_fast=True) # using fast defaultly, use_fast=True is not necessary\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ce57c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert/distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", use_fast=False) # using slow tokenizer\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "811528bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.8 ms, sys: 3.52 ms, total: 99.4 ms\n",
      "Wall time: 97.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(1_000):\n",
    "    fast_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1a70d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 258 ms, sys: 3.66 ms, total: 262 ms\n",
      "Wall time: 261 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(1_000):\n",
    "    slow_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfdcd93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 2.61 ms, total: 154 ms\n",
      "Wall time: 16.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fast_tokenizer([sentence] * 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f009ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 240 ms, sys: 295 μs, total: 241 ms\n",
      "Wall time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = slow_tokenizer([sentence] * 1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca714b0",
   "metadata": {},
   "source": [
    "## Offset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "274e9d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2000, 2022, 2030, 2025, 2000, 2022, 1010, 2023, 2003, 1037, 3160, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 5), (6, 8), (9, 12), (13, 15), (16, 18), (18, 19), (20, 24), (25, 27), (28, 29), (30, 38), (38, 39), (0, 0)]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sentence, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d692fc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14f08476",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m inputs = \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyFiles/GitHub/demos/transformers/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2910\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2909\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2912\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyFiles/GitHub/demos/transformers/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3020\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2999\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3000\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n\u001b[32m   3019\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyFiles/GitHub/demos/transformers/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3095\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3066\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3067\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3068\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3083\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3086\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3087\u001b[39m     padding=padding,\n\u001b[32m   3088\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3092\u001b[39m     **kwargs,\n\u001b[32m   3093\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3095\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyFiles/GitHub/demos/transformers/.venv/lib/python3.12/site-packages/transformers/tokenization_utils.py:792\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    786\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    787\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    788\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m integers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    789\u001b[39m             )\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    795\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMore information on available tokenizers at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m    800\u001b[39m first_ids = get_input_ids(text)\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sentence, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79763db5",
   "metadata": {},
   "source": [
    "# Load Special Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1591f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "973b9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/thu-coai/ShieldLM-6B-chatglm3:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='thu-coai/ShieldLM-6B-chatglm3', vocab_size=64798, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('thu-coai/ShieldLM-6B-chatglm3', trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3068e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./outs/special_tokenizer/tokenizer_config.json',\n",
       " './outs/special_tokenizer/special_tokens_map.json',\n",
       " './outs/special_tokenizer/tokenizer.model',\n",
       " './outs/special_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./outs/special_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03466e4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The repository ./outs/special_tokenizer contains custom code which must be executed to correctly load the model. You can inspect the repository content at /home/ubuntu/MyFiles/GitHub/demos/transformers/outs/special_tokenizer .\n You can inspect the repository content at https://hf.co/./outs/special_tokenizer.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./outs/special_tokenizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyFiles/GitHub/demos/transformers/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1093\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1092\u001b[39m         upstream_repo = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupstream_repo\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m   1098\u001b[39m     tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MyFiles/GitHub/demos/transformers/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:748\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code, error_message, upstream_repo)\u001b[39m\n\u001b[32m    745\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    749\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m You can inspect the repository content at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    750\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    751\u001b[39m     )\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: The repository ./outs/special_tokenizer contains custom code which must be executed to correctly load the model. You can inspect the repository content at /home/ubuntu/MyFiles/GitHub/demos/transformers/outs/special_tokenizer .\n You can inspect the repository content at https://hf.co/./outs/special_tokenizer.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./outs/special_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f27aff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./outs/special_tokenizer', trust_remote_code=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
